---
  title: "happiness_project"
output: html_document
---
  
  
  
```{r happiness, echo=FALSE}
#LOADING DATA AND DOWNLOADING PACKAGES

data = read.csv('https://raw.githubusercontent.com/arunagossai/happiness_project_R/master/happiness_data.csv', header = TRUE)
head(data)
summary(data)
dim(data)
#install.packages('fastDummies')
#install.packages('qdapTools')
#install.packages('glmnet')
#install.packages('tseries')
library(tseries)
library(fastDummies)
library(qdapTools)
library(glmnet)
library(rpart)
library(rpart.plot)
library(e1071)
```



```{r happiness, echo=FALSE}
#VARIABLE CREATION AND PREPROCESSING

#Creating a key for the regions, will use later
regionkey = levels(data$region)
regionvalues = c(1:10)
key = data.frame(regionkey,regionvalues)

#Changing the region from categorical to numeric
data$region <- as.numeric(data$region)

#Change getting rid of countries with no observations, and countries with no democracy value
df1  = subset(data, country != "Kosovo"  & country !="Taiwan" & country!="Sudan" & democracy!= 'NA')
paste( dim(data)[1] - dim(df1)[1], "observations lost")

#Taking the mean of each column by country. Changes dataset from pooled cross-sectional to cross-sectional 
df2 <- aggregate.data.frame(df1[-2], by = list(df1$country), mean)
paste( dim(df1)[1] - dim(df2)[1], "observations lost")

#adding a column for the region name
rname = lookup(df2$region,key$regionvalues,key$regionkey)
df = data.frame(df2,rname)

#Creating dummy variables from the region name
df_dum <- dummy_cols(df, select_columns = "rname")

#testing for multicollinearity excluding regions and year from matrix
cor(df[6:15])
#serious issues with multicollinearity, dropping the problem variables
df$men_edu <- NULL
df$sanitation <- NULL
df$elder_child <- NULL
df$child_mortality <- NULL
df_dum$men_edu <- NULL
df_dum$sanitation <- NULL
df_dum$elder_child <- NULL
df_dum$child_mortality <- NULL

#dropping variables that are not needed
df$year <- NULL
df$ï..id <- NULL
df$region <- NULL
df_dum$year <- NULL
df_dum$ï..id <- NULL
df_dum$region <- NULL
df_dum$rname <- NULL  #getting rid of one dummy variable to prevent multicollinearity

#creating binary 'very happy' variables for classification models
df_dum$veryhappy <- ifelse(df_dum$happiness >= 6.5,1,0)
df$veryhappy <- ifelse(df$happiness >= 6.5,1,0)

###NOTE####
#we now have two datasets. One has a column for region as a dummy variable, one does not. you can view them with the code below. Please remember that we have decided NOT to get rid of the  countries with outliers. Your two options are: replacing outliers with NAs or keeping them as is
```





```{r happiness, echo=FALSE}
#START YOUR ANALYSIS HERE
df1 = df_dum
dim(df1)

#relationships with independent
hist(df1$happiness)
jarque.bera.test(df1$happiness) #happiness is not normally distributed
plot(df1$women_edu,df1$happiness)
plot(log(df1$women_edu),df1$happiness)#women_edu seems to fit better with log(women_edu)

plot(df1$democracy,df1$happiness) #democracy variable seems to have a linear relationship
plot(df1$gini,df1$happiness) 
plot(log(df1$gini),df1$happiness) #gini seems uncorrelated to happiness
plot(df1$gini^2,df1$happiness) #the log or squared of gini does not help the fit

plot(df1$refugee,df1$happiness) 
plot(df1$refugee,df1$happiness, xlim = c(0,1))
plot(log(df1$refugee + 1),df1$happiness)
plot(log(df1$refugee + 1),df1$happiness, xlim = c(0,1)) # refugee share does not seem to be correlated, log helps with variance

plot(df1$pop_den,df1$happiness)
plot(log(df1$pop_den),df1$happiness) # pop not seems uncorrelated. log noticably reduces variance

plot(df1$labour,df1$happiness)
plot(log(df1$labour),df1$happiness)
plot(df1$labour,log(df1$happiness)) #labour seems uncorrelated to happiness

#TRANSFORMING VARIABLES
df1$refugee <- log(df1$refugee+1)
df1$women_edu <- log(df1$women_edu)
df1$pop_den <- log(df1$pop_den)
df1$refugee[df1$refugee > 1] <- mean(df1$refugee)
pairs(df1[2:8])
```





```{r}
#MODEL 1: LINEAR MODEL WITH ALL VARIABLES
set.seed(111)
n = nrow(df1)
Index = sample(1:n, size = round(0.7*n), replace=FALSE)
train1 = df1[Index,]
test1 = df1[-Index,]  

M1 = lm(happiness ~ ., train1[2:17])
pred_1 = predict(M1, test1[3:17])
RMSE_OUT_1 = sqrt(sum((pred_base-test1$happiness)^2)/length(pred_1))
RMSE_OUT_1
summary(M1)
```





```{r}
#MODEL 2
#TESTING RIDGE, LASSO AND HYBRID PENALTIES
results <- c()
alpha = c(0,.25,.5,.75,1)

for(i in 1:5){
  M2 = cv.glmnet(as.matrix(train1[3:17]),train1$happiness, alpha = alpha[i], nfolds = 5)
  
  pred_in_2 = predict(M2, as.matrix(train1[3:17]), s = 'lambda.min')
  pred_out_2 = predict(M2, as.matrix(test1[3:17]), s = 'lambda.min')
  
  RMSE_IN_2 = sqrt(sum((pred_in_2-train1[,2])^2)/length(pred_in_2))
  RMSE_OUT_2 = sqrt(sum((pred_out_2-test1[,2])^2)/length(pred_out_2))
  
  results <- rbind(results,c(alpha[i],RMSE_IN_2,RMSE_OUT_2,M2$lambda.min))
  colnames(results)<-c("Alpha","RMSE_IN","RMSE_OUT","Lambda")
}
results
results[which(results[,3] == min(results[,3])),]


#MODEL 2: RIDGE REGRESSION WITH A 5 FOLD CROSS VALIDATION
M2 = cv.glmnet(as.matrix(train1[3:17]),train1$happiness, alpha = 0, nfolds = 5)
pred_out_2 = predict(M2, as.matrix(test1[3:17]), s = 'lambda.min')
plot(M2)
coef(M2, s = 'lambda.min')

#RESIDUAL ANALYSIS
#PLOTTING FITTED VALUES
plot(pred_out_2,test1$happiness, col = c(1,6))
plot(pred_out_2-test1$happiness)
abline(0,0,col='black')
hist(pred_out_2-test1$happiness)
summary(pred_out_2-test1$happiness)
jarque.bera.test(pred_out_2-test1$happiness) #null of the JB test is normally distributed
```



```{r}
#MODEL 5
#TESTING SVM WITH DIFFERENT GAMMA
SVM_results = c()
gamma = c(.001,.0005,.0001,.00005,.00001)
for (i in 1:5){
  SVM<-svm(veryhappy~ ., data = train1[3:19], kernel = "radial", gamma = gamma[i])
  
  SVM_IN =  predict(SVM, train1)
  SVM_OUT =  predict(SVM, test1)

  SVM_IN <- ifelse(SVM_IN >= .05,1,0)
  SVM_OUT <- ifelse(SVM_OUT >= .05,1,0)
  
  confusion_IN_5 = table(SVM_IN,train1$veryhappy)
  confusion_OUT_5 = table(SVM_OUT,test1$veryhappy)
  
  Accuracy_IN_5 = (confusion_IN_5[1,1]+confusion_IN_5[2,2])/length(SVM_IN)
  Accuracy_OUT_5 = (confusion_OUT_5[1,1]+confusion_OUT_5[2,2])/length(SVM_OUT)
  
  SVM_results <- rbind(SVM_results,c(gamma[i],Accuracy_IN_5,Accuracy_OUT_5))
  colnames(SVM_results)<-c("Gamma","Accuracy In","Accuracy Out")
}
SVM_results

#MODEL 5: SVM WITH GAMMA OF .0005
SVM<-svm(veryhappy~ ., data = train1[3:19], kernel = "radial", gamma = .0005)
hist(SVM$residuals)
jarque.bera.test(SVM$residuals)
summary(SVM$residuals)
```

